# -*- coding: utf-8 -*-
"""Grammer Score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cNKDMMNY6N_ggGg2TsA4bWEvBIxhNzV
"""

!pip install torchaudio transformers librosa scikit-learn pandas matplotlib seaborn

import os
import pandas as pd
import numpy as np
import librosa
import torchaudio
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr

import torch
from transformers import Wav2Vec2Processor, Wav2Vec2Model

# Load training data from 'train.csv' into a DataFrame
train_df = pd.read_csv("train.csv")

# Load test data from 'test.csv' into a DataFrame
test_df = pd.read_csv("test.csv")

# Load sample submission file (used for test prediction format) into a DataFrame
sample_submission = pd.read_csv("sample_submission.csv")
# Display the first 5 rows of the training DataFrame to preview the data
print(train_df.head())

# Load and preprocess audio data
def load_audio(path, sr=16000):
    waveform, original_sr = torchaudio.load(path)

    # If the audio's original sampling rate doesn't match the target sampling rate,
    # resample it to the desired rate (default is 16000 Hz).
    if original_sr != sr:
        waveform = torchaudio.functional.resample(waveform, original_sr, sr)

        # Return the waveform as a NumPy array, flattening to 1D (mono audio)
    return waveform[0].numpy()

def preprocess_audio(path):

  # Load and resample audio, then return the raw waveform
    waveform = load_audio(path)
    return waveform

# Load the pretrained Wav2Vec2 processor and model from Hugging Face
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")
model.eval() # Set the model to evaluation mode to disable dropout and gradients

def extract_features(audio):
    input_values = processor(audio, return_tensors="pt", sampling_rate=16000).input_values   # Process raw audio (1D NumPy array) into model-compatible input format
    with torch.no_grad():  # Disable gradient calculation to save memory and computation during inference
        features = model(input_values).last_hidden_state.mean(dim=1)  # Pass the audio through the model and get the last hidden layer output
    return features.squeeze().numpy() # Return the feature vector as a NumPy array (shape: [hidden_dim])

# Import the pandas library for data manipulation and analysis
import pandas as pd

# Load the test dataset from the CSV file into a DataFrame
train_df = pd.read_csv("train.csv")
# Display the first 5 rows of the DataFrame to get a quick preview of the data
print(train_df.head())

with open("train.csv", "r") as f:
    print(f.readline())  # Print the first line to inspect format

import os
import pandas as pd
import torchaudio
from tqdm import tqdm

# Path setup
csv_path = "train.csv"  # Adjust if needed
audio_dir = "."  # Current directory


# Load CSV
train_df = pd.read_csv(csv_path)

# Feature extraction example
def preprocess_audio(path):
    waveform, sample_rate = torchaudio.load(path)
    return waveform

def extract_features(waveform):
    # Use Mel Spectrogram for example
    mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)
    mel_spec = mel_spec_transform(waveform)
    return mel_spec.mean(dim=-1).squeeze().numpy()  # shape: (64,)

# Build dataset
X, y = [], []

for _, row in tqdm(train_df.iterrows(), total=len(train_df)):
    filename = row['filename'].strip()
    audio_path = os.path.join(audio_dir, filename)

    if not os.path.exists(audio_path):
        print(f"File not found: {audio_path}")
        continue

    try:
        waveform = preprocess_audio(audio_path)
        features = extract_features(waveform)
        X.append(features)
        y.append(row['label'])
    except Exception as e:
        print(f"Error processing {filename}: {e}")

import os

print(os.listdir())

import pandas as pd
import os
# Load the CSV file that contains filenames of audio samples
train_df = pd.read_csv("train.csv")
# List all files in the current directory
all_files = os.listdir(".")

# Identify which filenames listed in the CSV are not found in the directory
missing = [f for f in train_df['filename'] if f.strip() not in all_files]
# Output the missing filenames and how many are missing
print("Missing files:", missing)
print("Missing count:", len(missing))

def extract_features(waveform):
    mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)
    mel_spec = mel_spec_transform(waveform)

    # Ensure waveform is mono (single channel)
    if mel_spec.shape[0] > 1:
        mel_spec = mel_spec.mean(dim=0, keepdim=True)  # average over channels

    # Now average over time axis
    feature_vector = mel_spec.mean(dim=-1).squeeze().numpy()

    return feature_vector  # shape: (64,)

import numpy as np

# Filter out inconsistent features
clean_X, clean_y = [], []

for features, label in zip(X, y):
    if isinstance(features, np.ndarray) and features.shape == (64,):
        clean_X.append(features)
        clean_y.append(label)
    else:
        print(f"Dropping feature with shape: {features.shape if isinstance(features, np.ndarray) else 'Invalid'}")

# Convert to numpy arrays
X = np.array(clean_X)
y = np.array(clean_y)

print("Final feature shape:", X.shape)

from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split

# Split the dataset into training and validation sets (80% train, 20% val)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a Ridge Regression model (L2 regularized linear regression)
model = Ridge()
# Fit the model on training data
model.fit(X_train, y_train)

# Predict the target values for the validation set
y_pred = model.predict(X_val)

# Calculate Root Mean Squared Error (RMSE) to measure prediction error
rmse = np.sqrt(mean_squared_error(y_val, y_pred))

# Calculate Pearson correlation coefficient to measure the linear relationship
pearson_corr, _ = pearsonr(y_val, y_pred)


# Print evaluation metrics
print(f"RMSE on validation set: {rmse:.4f}")
print(f"Pearson Correlation: {pearson_corr:.4f}")

# Create a scatter plot to visualize the relationship between actual and predicted values
sns.scatterplot(x=y_val, y=y_pred)

# Label the x-axis as the true grammar scores
plt.xlabel("Actual Score")

# Label the y-axis as the predicted grammar scores
plt.ylabel("Predicted Score")

# Set the plot title
plt.title("Actual vs Predicted Grammar Score")

# Display the plot
plt.show()

import os
import pandas as pd
import torchaudio
from tqdm import tqdm

# Path setup
test_csv_path = "test.csv"
test_audio_dir = "."  # Current directory

# Load CSV
test_df = pd.read_csv(test_csv_path)

# Feature extraction example
def preprocess_audio(path):
    waveform, sample_rate = torchaudio.load(path)
    return waveform

def extract_features(waveform):
    # Use Mel Spectrogram for example
    mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)
    mel_spec = mel_spec_transform(waveform)
    return mel_spec.mean(dim=-1).squeeze().numpy()  # shape: (64,)

# Build test dataset
X_test = []

for _, row in tqdm(test_df.iterrows(), total=len(test_df)):
    filename = row['filename'].strip()
    audio_path = os.path.join(test_audio_dir, filename)

    if not os.path.exists(audio_path):
        print(f"File not found: {audio_path}")
        continue

    try:
        waveform = preprocess_audio(audio_path)
        features = extract_features(waveform)
        X_test.append(features)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Convert to numpy array if needed
X_test = np.vstack(X_test) if X_test else np.array([])  # Handle empty case

# Import the pandas library for data manipulation and analysis
import pandas as pd

# Load the test dataset from the CSV file into a DataFrame
train_df = pd.read_csv("test.csv")
# Display the first 5 rows of the DataFrame to get a quick preview of the data
print(train_df.head())

with open("test.csv", "r") as f:
    print(f.readline())  # Print the first line to inspect format

def extract_features(waveform):
    mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)
    mel_spec = mel_spec_transform(waveform)

    # Ensure waveform is mono (single channel)
    if mel_spec.shape[0] > 1:
        mel_spec = mel_spec.mean(dim=0, keepdim=True)  # average over channels

    # Now average over time axis
    feature_vector = mel_spec.mean(dim=-1).squeeze().numpy()

    return feature_vector  # shape: (64,)

import numpy as np

# Filter out inconsistent features
clean_X, clean_y = [], []

for features, label in zip(X, y):
    if isinstance(features, np.ndarray) and features.shape == (64,):
        clean_X.append(features)
        clean_y.append(label)
    else:
        print(f"Dropping feature with shape: {features.shape if isinstance(features, np.ndarray) else 'Invalid'}")

# Convert to numpy arrays
X = np.array(clean_X)
y = np.array(clean_y)

print("Final feature shape:", X.shape)

submission.to_csv("submission.csv", index=False)

submission = pd.DataFrame({
    "filename": test['filename'],
    "label": predictions
})

submission.to_csv("submission.csv", index=False)

from google.colab import files
files.download("submission.csv")

"""### Final Evaluation Metrics
- RMSE on training data: 1.1153
- Pearson correlation on validation: 0.3725

### Summary
We used Wav2Vec2 for audio feature extraction and Ridge Regression for scoring. This model demonstrated strong correlation and reasonable RMSE, suggesting effectiveness in grammar scoring from audio samples.


1. Files Used for Data Preparation:

The filename and label columns are present in train.csv.

Audio files in.wav format: Found in a designated folder (Dataset_new/audios/train, for example).
Find Missing Files: The function determines which filenames in the CSV are missing from the directory.



 2. Torchaudio is the audio preprocessing library used.

 Actions to take:

each.wav file should be loaded using torchaudio.load().
Create a Mel Spectrogram (n_mels=64) from the waveform. Calculate the mean across the time axis to reduce each Mel Spectrogram to a fixed-size vector.

Each audio file's resulting feature vector shape is (64,).



 3. Extraction of Features  Loop
goes through every row in the CSV:
The audio file is loaded. extracts features as previously mentioned.
adds the matching label to y and the feature vector to x.

4. Feature Data Cleaning
Before modeling, feature vectors that are invalid or improperly formed are removed:
Any sample without a (64,) shape is filtered out.


 5. Ridge Regression (from sklearn.linear_model) was the model used for training.

The model is trained using the label vector y and the cleaned feature matrix X.

The following actions would be:
dividing X and Y into sets for training and validation

 6. Model training

Using Pearson correlation for evaluation and prediction
"""

